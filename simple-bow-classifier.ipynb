{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da8b6f1",
   "metadata": {
    "papermill": {
     "duration": 0.007991,
     "end_time": "2022-11-23T19:35:45.662756",
     "exception": false,
     "start_time": "2022-11-23T19:35:45.654765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Simple text classifier\n",
    "\n",
    "Build a simple text classifier using BoW (Bag of Words) to convert text to numerical values and classify a text based on a specific topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74465b",
   "metadata": {
    "papermill": {
     "duration": 0.006707,
     "end_time": "2022-11-23T19:35:45.676708",
     "exception": false,
     "start_time": "2022-11-23T19:35:45.670001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5b2f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:35:45.693025Z",
     "iopub.status.busy": "2022-11-23T19:35:45.692469Z",
     "iopub.status.idle": "2022-11-23T19:35:46.596800Z",
     "shell.execute_reply": "2022-11-23T19:35:46.595783Z"
    },
    "papermill": {
     "duration": 0.917332,
     "end_time": "2022-11-23T19:35:46.601165",
     "exception": false,
     "start_time": "2022-11-23T19:35:45.683833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence without encoding:\n",
      "I love the book\n",
      "\n",
      "\n",
      "Unigram vectorizer learned n-grams:\n",
      "['book', 'great', 'is', 'love', 'nice', 'shirt', 'shoes', 'the', 'this', 'your']\n",
      "\n",
      "\n",
      "Bigram vectorizer learned n-grams:\n",
      "['great book', 'is great', 'is nice', 'love the', 'love your', 'nice shirt', 'the book', 'this is', 'your shoes']\n",
      "\n",
      "\n",
      "Unigram + bigram vectorizer learned n-grams:\n",
      "['book', 'great', 'great book', 'is', 'is great', 'is nice', 'love', 'love the', 'love your', 'nice', 'nice shirt', 'shirt', 'shoes', 'the', 'the book', 'this', 'this is', 'your', 'your shoes']\n",
      "\n",
      "\n",
      "First sentence unigrams: [1 0 0 1 0 0 0 1 0 0]\n",
      "First sentence bigrams: [0 0 0 1 0 0 1 0 0]\n",
      "First sentence uni + bigrams: [1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get dummy data\n",
    "X = ['I love the book', 'This is a great book', 'This is a nice shirt', 'I love your shoes']\n",
    "y = ['book', 'book', 'clothes', 'clothes']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_uni = CountVectorizer(binary=True, # binary = wether to apply a binary encoding or keep the count of ocurences\n",
    "                                 ngram_range=(1, 1),) # (1, 1) = Unigram only, (1, 2) = Uni + Bigram, (2, 2) = Bigram only \n",
    "# Fit to the data\n",
    "X_uni = vectorizer_uni.fit_transform(X)\n",
    "\n",
    "vectorizer_bi = CountVectorizer(binary=True,\n",
    "                                ngram_range=(2, 2))\n",
    "X_bi = vectorizer_bi.fit_transform(X)\n",
    "\n",
    "vectorizer_uni_bi = CountVectorizer(binary=True,\n",
    "                                    ngram_range=(1, 2))\n",
    "X_uni_bi = vectorizer_uni_bi.fit_transform(X)\n",
    "\n",
    "print('First sentence without encoding:')\n",
    "print(X[0])\n",
    "print('\\n')\n",
    "\n",
    "print('Unigram vectorizer learned n-grams:')\n",
    "print(vectorizer_uni.get_feature_names())\n",
    "print('\\n')\n",
    "\n",
    "print('Bigram vectorizer learned n-grams:')\n",
    "print(vectorizer_bi.get_feature_names())\n",
    "print('\\n')\n",
    "\n",
    "print('Unigram + bigram vectorizer learned n-grams:')\n",
    "print(vectorizer_uni_bi.get_feature_names())\n",
    "print('\\n')\n",
    "\n",
    "print(f'First sentence unigrams: {X_uni.toarray()[0]}')\n",
    "print(f'First sentence bigrams: {X_bi.toarray()[0]}')\n",
    "print(f'First sentence uni + bigrams: {X_uni_bi.toarray()[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b0f61",
   "metadata": {
    "papermill": {
     "duration": 0.004392,
     "end_time": "2022-11-23T19:35:46.610245",
     "exception": false,
     "start_time": "2022-11-23T19:35:46.605853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Simple classifier using LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391d2fa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:35:46.620971Z",
     "iopub.status.busy": "2022-11-23T19:35:46.620561Z",
     "iopub.status.idle": "2022-11-23T19:35:46.730952Z",
     "shell.execute_reply": "2022-11-23T19:35:46.729823Z"
    },
    "papermill": {
     "duration": 0.119747,
     "end_time": "2022-11-23T19:35:46.734524",
     "exception": false,
     "start_time": "2022-11-23T19:35:46.614777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['book', 'clothes', 'clothes'], dtype='<U7')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(X_uni, y)\n",
    "\n",
    "X_test = ['I like this book', 'I love your shirt', 'Nice shoes']\n",
    "\n",
    "clf.predict(vectorizer_uni.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d233e8c8",
   "metadata": {
    "papermill": {
     "duration": 0.004825,
     "end_time": "2022-11-23T19:35:46.744173",
     "exception": false,
     "start_time": "2022-11-23T19:35:46.739348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## BoW issues\n",
    "\n",
    "1) The more n-grams you have the more biased the classifier may be. For instance, if you fit a 10-gram BoW then the classifier would have trouble generalizing 10 worded sentences.\n",
    "\n",
    "2) If a word is not present to the BoW dictionary then the encoding would fail.\n",
    "\n",
    "3) The sparsity of the fitted data may increase the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4143955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:35:46.755051Z",
     "iopub.status.busy": "2022-11-23T19:35:46.754682Z",
     "iopub.status.idle": "2022-11-23T19:35:46.764208Z",
     "shell.execute_reply": "2022-11-23T19:35:46.762982Z"
    },
    "papermill": {
     "duration": 0.017494,
     "end_time": "2022-11-23T19:35:46.766360",
     "exception": false,
     "start_time": "2022-11-23T19:35:46.748866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['book'], dtype='<U7')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(vectorizer_uni.transform(['I love the boots'])) # Boots is not present to the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeef989",
   "metadata": {
    "papermill": {
     "duration": 0.004599,
     "end_time": "2022-11-23T19:35:46.775814",
     "exception": false,
     "start_time": "2022-11-23T19:35:46.771215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Word vectors\n",
    "\n",
    "Using spacy's trained pipeline that has pre-trained transformations that output a vector for each word which is a spacial representation of that word into a (300,) sized vector. For sentences, it computes the mean for each word (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e33719b",
   "metadata": {
    "papermill": {
     "duration": 0.00506,
     "end_time": "2022-11-23T19:35:46.787251",
     "exception": false,
     "start_time": "2022-11-23T19:35:46.782191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Downloading the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c963ba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:35:46.799250Z",
     "iopub.status.busy": "2022-11-23T19:35:46.798336Z",
     "iopub.status.idle": "2022-11-23T19:36:13.827868Z",
     "shell.execute_reply": "2022-11-23T19:36:13.826608Z"
    },
    "papermill": {
     "duration": 27.038554,
     "end_time": "2022-11-23T19:36:13.830712",
     "exception": false,
     "start_time": "2022-11-23T19:35:46.792158",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.3.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.3.0/en_core_web_md-3.3.0-py3-none-any.whl (33.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-md==3.3.0) (3.3.1)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.17)\r\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (59.8.0)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.6.2)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.1.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.7.9)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.10)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.9)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.28.1)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.10.1)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.7)\r\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.1.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (21.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.4.5)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.21.6)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.8.2)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.64.0)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3.0)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.8)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.8.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\r\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (5.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2022.9.24)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.26.12)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.13.0)\r\n",
      "Installing collected packages: en-core-web-md\r\n",
      "Successfully installed en-core-web-md-3.3.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_md')\r\n"
     ]
    }
   ],
   "source": [
    "# Download vectors\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b787b",
   "metadata": {
    "papermill": {
     "duration": 0.007097,
     "end_time": "2022-11-23T19:36:13.845502",
     "exception": false,
     "start_time": "2022-11-23T19:36:13.838405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the pipeline and trying to find some similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40091822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:36:13.862275Z",
     "iopub.status.busy": "2022-11-23T19:36:13.861237Z",
     "iopub.status.idle": "2022-11-23T19:36:19.720150Z",
     "shell.execute_reply": "2022-11-23T19:36:19.717727Z"
    },
    "papermill": {
     "duration": 5.870413,
     "end_time": "2022-11-23T19:36:19.723054",
     "exception": false,
     "start_time": "2022-11-23T19:36:13.852641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06928015\n",
      "1.0\n",
      "0.71957725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Spacy has an english trained pipeline\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def similarity(a, b):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between strings.\n",
    "    \"\"\"\n",
    "    # Get vector representation\n",
    "    a_vec = nlp(a).vector\n",
    "    b_vec = nlp(b).vector\n",
    "    \n",
    "    return np.dot(a_vec, b_vec.T)/(np.linalg.norm(a_vec)*np.linalg.norm(b_vec))\n",
    "\n",
    "print(similarity('cat', 'book')) # Not similar at all\n",
    "print(similarity('cat', 'dog')) # Really similar\n",
    "print(similarity('book', 'library')) # Pretty similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092bff4",
   "metadata": {
    "papermill": {
     "duration": 0.006982,
     "end_time": "2022-11-23T19:36:19.737696",
     "exception": false,
     "start_time": "2022-11-23T19:36:19.730714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Retraining the simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fae96c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:36:19.754993Z",
     "iopub.status.busy": "2022-11-23T19:36:19.753610Z",
     "iopub.status.idle": "2022-11-23T19:36:19.798642Z",
     "shell.execute_reply": "2022-11-23T19:36:19.797531Z"
    },
    "papermill": {
     "duration": 0.056398,
     "end_time": "2022-11-23T19:36:19.801431",
     "exception": false,
     "start_time": "2022-11-23T19:36:19.745033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['clothes'], dtype='<U7')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get vector representation for each sentence\n",
    "X_vectors = np.array([nlp(text).vector for text in X])\n",
    "\n",
    "# Retrain the classifier\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(X_vectors, y)\n",
    "\n",
    "# The simple classifier predicted \"book\" for the sentence\n",
    "# Now eventhough the word \"boots\" is not in our vocab\n",
    "# the classifier is able to correctly predict the proper label\n",
    "clf.predict(np.array([nlp('I love the boots').vector]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28b6d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:36:19.817843Z",
     "iopub.status.busy": "2022-11-23T19:36:19.817273Z",
     "iopub.status.idle": "2022-11-23T19:36:19.833590Z",
     "shell.execute_reply": "2022-11-23T19:36:19.832491Z"
    },
    "papermill": {
     "duration": 0.027253,
     "end_time": "2022-11-23T19:36:19.836109",
     "exception": false,
     "start_time": "2022-11-23T19:36:19.808856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['book'], dtype='<U7')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another slightly tricky example\n",
    "clf.predict(np.array([nlp('I love going to the library').vector]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc281a7",
   "metadata": {
    "papermill": {
     "duration": 0.007176,
     "end_time": "2022-11-23T19:36:19.850775",
     "exception": false,
     "start_time": "2022-11-23T19:36:19.843599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Embeddings issues\n",
    "\n",
    "1) Sometimes squashing a whole sentence/text/document into a single vector representation may lose some individual word meaning.\n",
    "\n",
    "2) Word embeddings don't take to account the word positioning, which may add a total different meaning to the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9f2335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:36:19.867729Z",
     "iopub.status.busy": "2022-11-23T19:36:19.866766Z",
     "iopub.status.idle": "2022-11-23T19:36:19.889370Z",
     "shell.execute_reply": "2022-11-23T19:36:19.888225Z"
    },
    "papermill": {
     "duration": 0.033829,
     "end_time": "2022-11-23T19:36:19.891937",
     "exception": false,
     "start_time": "2022-11-23T19:36:19.858108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same words rearanged into different sentences\n",
    "sentence_1 = 'Her dog has gone for a walk with Mary'\n",
    "sentence_2 = 'Mary has gone for a walk with her dog'\n",
    "\n",
    "similarity(sentence_1, sentence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6597d807",
   "metadata": {
    "papermill": {
     "duration": 0.007257,
     "end_time": "2022-11-23T19:36:19.906862",
     "exception": false,
     "start_time": "2022-11-23T19:36:19.899605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stemming/Lemmatization\n",
    "\n",
    "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa6dcebd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:36:19.923965Z",
     "iopub.status.busy": "2022-11-23T19:36:19.922912Z",
     "iopub.status.idle": "2022-11-23T19:36:21.140409Z",
     "shell.execute_reply": "2022-11-23T19:36:21.139354Z"
    },
    "papermill": {
     "duration": 1.228817,
     "end_time": "2022-11-23T19:36:21.143163",
     "exception": false,
     "start_time": "2022-11-23T19:36:19.914346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff16a733",
   "metadata": {
    "papermill": {
     "duration": 0.008088,
     "end_time": "2022-11-23T19:36:21.159873",
     "exception": false,
     "start_time": "2022-11-23T19:36:21.151785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde3ec0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:36:21.178343Z",
     "iopub.status.busy": "2022-11-23T19:36:21.177940Z",
     "iopub.status.idle": "2022-11-23T19:36:21.196118Z",
     "shell.execute_reply": "2022-11-23T19:36:21.194806Z"
    },
    "papermill": {
     "duration": 0.031465,
     "end_time": "2022-11-23T19:36:21.199512",
     "exception": false,
     "start_time": "2022-11-23T19:36:21.168047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read the book\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "phrase = 'reading the books'\n",
    "words = word_tokenize(phrase) # Break the sentence into individual words\n",
    "stemmed_words = [stemmer.stem(word) for word in words] # Apply \n",
    "new_phrase = ' '.join(stemmed_words)\n",
    "print(new_phrase) # Expected \"read the book\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a9b68",
   "metadata": {
    "papermill": {
     "duration": 0.008102,
     "end_time": "2022-11-23T19:36:21.217190",
     "exception": false,
     "start_time": "2022-11-23T19:36:21.209088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d64e1698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T19:36:21.235193Z",
     "iopub.status.busy": "2022-11-23T19:36:21.234825Z",
     "iopub.status.idle": "2022-11-23T19:36:22.950942Z",
     "shell.execute_reply": "2022-11-23T19:36:22.948742Z"
    },
    "papermill": {
     "duration": 1.727853,
     "end_time": "2022-11-23T19:36:22.953273",
     "exception": false,
     "start_time": "2022-11-23T19:36:21.225420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the book\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "phrase = 'reading the books'\n",
    "words = word_tokenize(phrase) # Break the sentence into individual words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words] # Apply \n",
    "new_phrase = ' '.join(lemmatized_words)\n",
    "print(new_phrase) # Expected \"read the book\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd414649",
   "metadata": {
    "papermill": {
     "duration": 0.007688,
     "end_time": "2022-11-23T19:36:22.968847",
     "exception": false,
     "start_time": "2022-11-23T19:36:22.961159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.312587,
   "end_time": "2022-11-23T19:36:26.180228",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-23T19:35:37.867641",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
